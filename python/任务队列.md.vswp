vnote_backup_file_826537664 E:/OneDrive/blog.moyechen.cn/python/任务队列.md
# 任务队列


# 流程


## 最初版本
1.通过接口获取到今天的数据(.csv)

2. 执行程序


## aps版本

定义一个定时任务, 然后每天稳定拉取数据,到任务队列中, 程序读取此队列, 并将爬取结果保存到结果队列


### 四个进程
- 任务获取进程
- 爬虫进程
- 上传进程
- 监控进程


### 两个队列

- 任务队列
- 结果队列



这种方式看起来非常专业, 但是有时候临时需要跑一部分数据时, 就会很麻烦, 因此我想这在队列这里解决, 从python 自带的队列替换为 redis的list


## redis 版本


在将python 的队列更改为redis 后, 我又编写了 redis_push.py , 可以手动将文件中的数据推送到redis 队列中, 非常的好用

但是过了周六周日两天后, 我发现 APScheduler 又出现了bug, 没有按时拉取数据, 我不由得怀疑这个库的稳定性 ,因此我又回到了crontab


## crontab 版本


经过了近一个月, 多次的更改,我探究了最适合的方案

需要有以下几个优点

- 可随时跑残缺数据            python run.py file
- 网站变化频繁, 只需修改spider 文件即可, 比较方便


```

- source_data/      原始数据
- aaa/   爬虫aaa
    --  spider.py  爬虫主体, 仅负责爬取数据
    --  main.py     读取文件, 解析数据, 上传数据等等
    --  data/        爬到的数据
- bbb/
    --  spider.py
    --  main.py     
    --  data/
- download_data.py  拉取原始数据, 使用crontab 例行化

```